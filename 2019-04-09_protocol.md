# Protocol 2019-04-09, Tuesday

## General topics

* First meeting. Organizational questions.
* Starting point of our meetup: How science should be made? Why haven't methods in social sciences worked that well so far?

## Papers/Technicalities

* Meehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology. Journal of consulting and clinical Psychology, 46(4), 806.

## Protocol

* List of shortcuts used in the protocol
  * SoSci = social science(s)

### SESSION START

* Organizational questions
* Goals, format, place, etc.
  * ...are to be discussed further in private.
* Evtl. registering our meetup as Projektutorium in Wintersemester 2018/19 [Deadline application for HU Berlin: 2 June 2019]

### Discussion Paper Meehl

* Opening statement: Why (soft) psychology and social sciences in general failed to deliver theories/models of high or even any scientific/practical merit? How can we fix it?
	* Meehl's list of 20 points: Apart from all other problems, there is a number of intrinsic qualities of the object of study (namely, human behavior), that make it almost impossible for SoSci to develop such theories.
	* Plenum: The tragedy of this statement is that even if we organize our scientific endeavor in the best way (open science, pre-registration, etc.) and have unlimited resources (huge sample sizes, processing power, etc.), we would nevertheless not be able to overcome these intrinsic barriers.
	* Also see Meehl's conclusion/prediction in the end of the paper: Psychology will probably never solve this problem and we won't see any useful theories.
	* Some very good and tested theories in subfields such as perception psychology but findings rarely relevant for the social policies, for example.
* Historical remark
	* Question: Regarding the need for strict formal models - is this a new problem in psychology or has it always been a goal?
	* Plenum: The founders of (experimental) psychology, (e.g., Wundt, Helmholtz) explicitly wrote that we need to apply rigorous scientific/math methods of hard sciences to human behavior - thus, the new field of psychology. They wanted to go for that "risky" predictions and strong test of the theories.
	* In some aspects, we are even degrading from that viewpoint. Perhaps the development of computers, and thus statistical software, prevented us from complete failure, but it may end soon. 
	* Also, there is a trend now (cf. open science foundation, replication) that disproving/refuting a theory is even more.
	
### Possible solutions

* More variables in the model?
	* Roughly speaking, researchers could consider more variables in their models to check for all influences.
	* Contra-point: Yes, but what about all the problems that arise with that approach?
		* (Cf. Meehl's point about "sheer number of variables")
		* High-dimensional data --> multicollinearity, number of interaction terms, etc. You won't be able to look through all this.
		* Moreover, there is an infinite number of possible variables that you can theoretically construct and measure. How would you decide how many and which one to choose?
		* Argument: If we have infinite number of variables, and thus models/theories, you can not say that your theory is more "versimile," than others, because you haven't tested them all.
* More attention to auxialiaries?
	* See Meehl's argument that we always testing the conjunction and not the core theory only (cf. Dunheim-Ding paradox).
	* Point: Psychologist researchers do not pay enough attention to the auxiliary theories. If you, say, use some construct in your experiment, you must be able to have a good theory about it as well (why I used this measure? What are its qualities?) 
* Using an obligatory set of standards, at least within a (sub-)field?
  * Point: If there is a number of standards in a field that every researcher must follow, it may increase the quality of research.
    * Example: Every analysis in sociology must include control for genetics.
    * Side remark by a graduate of sociology: Almost no mention of genetics throughout the whole Bachelor in sociology (HU Berlin). Although, some large longitudinal studies (such as Socio-Economic Panel in Germany) now consider sequencing participants' genomes.
  * Counter-point: Will it not lead to dogmatism in science or persistence of outdated paradigms (cf. Thomas Kuhn)? Who will decide about the standards? What if these standards are inefficient/outdated or plain wrong?
* Testing a theory/construct across several methods, measurement instruments, statistical models, samples, etc.?
  * Point: Validation a construct/relationship across several methods/samples/etc. is a very strong test of and an argument for its validity.
  * See related terms: Model independence, cross-validation, triangulation
  * Very few examples of such constructs in (soft) psychology.
    * g factor of intelligence: A factor "explaining" ~50% of variance comes up almost in every test with abstract problems.
    * Semantic/lexical hypothesis of personality.
    
### Further points

* Distinction between two approaches: Testing a theory VS fitting a model to the data.
  * In psychology, we rarely test the theory, rather "just" collect data and fit a model.
  * How we could come to conclusion about the definitions in SoSci?
    * Compare Meehl's example of X ways of estimating the Avogadro number with terms from behavior science like "aggression"
    * Avogadro number is "always out there," it is a fundamental law in our nature (cf. Comte's hierarchy of science). Thus, it has been just "waiting to be discovered".

#### Reading suggestions
* Meehl, P. E. (1990). Appraising and amending theories: The strategy of Lakatosian defense and two principles that warrant it. *Psychological inquiry*, 1(2), 108-141.
	* Mentioned by one member as the paper on philosophy of science, if you had a chance to read only one.

